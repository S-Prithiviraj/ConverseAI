{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1fb31a-4073-4d6c-82ee-ff631d654a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip  \n",
    "from collections import Counter\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import noisereduce as nr\n",
    "\n",
    "# pkgs downloads\n",
    "nltk.download('punkt')  # For word/sentence tokenization\n",
    "nltk.download('stopwords')  # Stopwords for filtering common words\n",
    "nltk.download('vader_lexicon')  # For sentiment analysis\n",
    "\n",
    "# Load spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load NLTK stopwords list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe_audio(audio_path):\n",
    "    # Load the pre-trained ASR model and processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "    # Model Evaluation\n",
    "    model.eval()\n",
    "\n",
    "    # Read audio file\n",
    "    speech, samplerate = sf.read(audio_path)\n",
    "\n",
    "    # Ensure the audio is mono\n",
    "    if len(speech.shape) > 1:\n",
    "        speech = np.mean(speech, axis=1)\n",
    "\n",
    "    # Apply noise reduction\n",
    "    speech = nr.reduce_noise(y=speech, sr=samplerate, n_std_thresh_stationary=1.5)\n",
    "\n",
    "    # Convert sample rate if needed\n",
    "    if samplerate != 16000:\n",
    "        from scipy.signal import resample\n",
    "        num_samples = int(16000 * len(speech) / samplerate)\n",
    "        speech = resample(speech, num_samples)\n",
    "\n",
    "    # Segment audio if too long \n",
    "    max_length = 16000 * 30  # Here, 30 seconds\n",
    "    if len(speech) > max_length:\n",
    "        segments = [speech[i:i + max_length] for i in range(0, len(speech), max_length)]\n",
    "        transcriptions = []\n",
    "        for segment in segments:\n",
    "            input_values = processor(segment, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
    "            logits = model(input_values).logits\n",
    "            predicted_ids = logits.argmax(dim=-1)\n",
    "            transcription = processor.decode(predicted_ids[0])\n",
    "            transcriptions.append(transcription)\n",
    "        return ' '.join(transcriptions)\n",
    "\n",
    "    # Preprocess and transcribe\n",
    "    input_values = processor(speech, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = logits.argmax(dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription\n",
    "\n",
    "# Function to extract audio from a video file\n",
    "def extract_audio_from_video(video_path, output_audio_path=\"extracted_audio.wav\"):\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    audio = video_clip.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    return output_audio_path\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.lower()\n",
    "\n",
    "# Function to extract named entities (NER) as topics\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Function to analyze sentiment of the text\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_scores = sentiment_analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    # Classify sentiment as positive, negative, or neutral\n",
    "    if compound_score >= 0.05: #Sample Score\n",
    "        return \"Positive\", sentiment_scores\n",
    "    elif compound_score <= -0.05:\n",
    "        return \"Negative\", sentiment_scores\n",
    "    else:\n",
    "        return \"Neutral\", sentiment_scores\n",
    "\n",
    "# Function to visualize sentiment distribution\n",
    "def visualize_sentiment(sentiments):\n",
    "    categories = ['Positive', 'Negative', 'Neutral']\n",
    "    sentiment_counts = [sentiments.count(category) for category in categories]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(categories, sentiment_counts, color=['green', 'red', 'gray'])\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.show()\n",
    "\n",
    "# Function to extract and filter keywords by frequency\n",
    "def extract_keywords(text, frequency_threshold=3):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words_filtered = [word for word in words if word not in stopwords and len(word) > 3]  # Filter stopwords and short words\n",
    "    word_freq = Counter(words_filtered)\n",
    "\n",
    "    # Display Frequency words\n",
    "    keywords = {word: count for word, count in word_freq.items() if count >= frequency_threshold}\n",
    "    return keywords\n",
    "\n",
    "# Function to perform topic modeling and extract topics\n",
    "def perform_topic_modeling(texts, num_topics=3):\n",
    "    # Tokenize and preprocess texts\n",
    "    processed_texts = [preprocess_text(text).split() for text in texts]\n",
    "\n",
    "    # Create a dictionary and corpus for LDA\n",
    "    dictionary = Dictionary(processed_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Extract topics\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    return topics\n",
    "\n",
    "# Function to extract metadata: number of speakers, duration, number of words\n",
    "def extract_metadata(transcription, audio_file, language=\"English\"):\n",
    "    # Use the transcription for word count\n",
    "    word_count = len(transcription.split())\n",
    "\n",
    "    # Extract duration of audio\n",
    "    audio_info = sf.info(audio_file)\n",
    "    duration = audio_info.duration\n",
    "    num_speakers = 1  # Placeholder\n",
    "\n",
    "    metadata = {\n",
    "        \"Language\": language,\n",
    "        \"Duration (seconds)\": duration,\n",
    "        \"Number of Words\": word_count,\n",
    "        \"Number of Speakers\": num_speakers\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "# Function to display results and visualize the findings\n",
    "def display_results(transcription, named_entities, sentiments, keywords, metadata, topics):\n",
    "    # Display transcription\n",
    "    print(\"\\nTranscription:\\n\", transcription)\n",
    "\n",
    "    # Display named entities (topics)\n",
    "    print(\"\\nNamed Entities (Main Topics):\")\n",
    "    for entity in named_entities:\n",
    "        print(f\"Entity: {entity[0]}, Type: {entity[1]}\")\n",
    "\n",
    "    # Display sentiment analysis\n",
    "    print(\"\\nSentiment Analysis:\")\n",
    "    for sentence, sentiment in sentiments:\n",
    "        print(f\"Sentence: {sentence}\\nSentiment: {sentiment[0]}, Scores: {sentiment[1]}\\n\")\n",
    "\n",
    "    # Display keyword frequency\n",
    "    print(\"\\nKeyword Frequency:\")\n",
    "    for word, freq in keywords.items():\n",
    "        print(f\"Word: {word}, Frequency: {freq}\")\n",
    "\n",
    "    # Display topics\n",
    "    print(\"\\nTopics:\")\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "\n",
    "    # Display metadata\n",
    "    print(\"\\nConversation Metadata:\")\n",
    "    for key, value in metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # Visualize sentiment distribution\n",
    "    sentiment_list = [s[0] for s in sentiments]\n",
    "    visualize_sentiment(sentiment_list)\n",
    "\n",
    "# Main function \n",
    "def main(file_path):\n",
    "    # Check if the file is a video / audio\n",
    "    if file_path.lower().endswith(('.mp4', '.mkv', '.avi', '.mov')):  # Video file formats\n",
    "        print(\"Processing video file...\")\n",
    "        audio_file = extract_audio_from_video(file_path)\n",
    "    else:\n",
    "        print(\"Processing audio file...\")\n",
    "        audio_file = file_path\n",
    "\n",
    "    # Step 1: Transcribe the audio\n",
    "    transcription = transcribe_audio(audio_file)\n",
    "\n",
    "    # Step 2: Preprocess the text\n",
    "    cleaned_text = preprocess_text(transcription)\n",
    "\n",
    "    # Step 3: Extract named entities (NER)\n",
    "    named_entities = extract_named_entities(cleaned_text)\n",
    "\n",
    "    # Step 4: Sentiment analysis for each sentence\n",
    "    sentences = cleaned_text.split(\".\")  # Split text into sentences\n",
    "    sentiments = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():  # Skip empty sentences\n",
    "            sentiment = analyze_sentiment(sentence)\n",
    "            sentiments.append((sentence, sentiment))\n",
    "\n",
    "    # Step 5: Extract keyword frequencies\n",
    "    keywords = extract_keywords(cleaned_text)\n",
    "\n",
    "    # Step 6: Perform topic modeling\n",
    "    texts = [cleaned_text]  \n",
    "    topics = perform_topic_modeling(texts)\n",
    "\n",
    "    # Step 7: Extract metadata\n",
    "    metadata = extract_metadata(transcription, audio_file)\n",
    "\n",
    "    # Step 8: Display results\n",
    "    display_results(transcription, named_entities, sentiments, keywords, metadata, topics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'C:/Users/Admin/Desktop/Project-2/SampleOrderTaking_CustomerSupport.mp4'  #mp4, mp3, wav..\n",
    "    main(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
